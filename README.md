# ğŸ§ â€¯Blickshift Assistant â€“Â A Metacognitive, Truthâ€‘Aware Conversational AI

> **Oneâ€‘sentence pitch:**  *Plan*Â â†’Â *Check*Â â†’Â *Speak.*  Blickshift thinks before it talks, so you get answers that are both fluent **and** factâ€‘checked â€“ in text **or voice** â€“Â all on your local machine.

---

## ğŸ“œâ€¯TableÂ ofÂ Contents

1. [Why Blickshift?](#why-blickshift)
2. [System Overview](#system-overview)
3. [QuickÂ Start](#quick-start)
4. [Demo Scenarios](#demo-scenarios)
5. [Project Layout](#project-layout)
6. [Extending & Research Hooks](#extending--research-hooks)
7. [FAQ](#faq)
8. [Roadmap](#roadmap)
9. [License](#license)

---

## WhyÂ Blickshift?

LLMs are eloquent but unreliable â€“ they hallucinate, interrupt, and ramble.  **Blickshift** tackles all three by borrowing from *metacognition*:

| Human faculty                                                              | Implementation                                              | Payâ€‘off                                               |
| -------------------------------------------------------------------------- | ----------------------------------------------------------- | ----------------------------------------------------- |
| **Predictionâ€‘error monitoring** â€“ sense when a thought feels â€œoffâ€         | Tokenâ€‘level surprise hook in the decoder                    | Catches nonsensical continuations early               |
| **Epistemic feelings** â€“ gutâ€‘level confidence, relevance, â€œis it my turn?â€ | Fast heuristics: semantic similarity, VAD silence, KG match | Keeps chat on topic & timed like a human conversation |
| **Conceptual introspection** â€“ explicit reasoning & correction             | LargeÂ ConceptÂ Model (LCM) plans â†’ NLI fact verifier         | Makes sure content is logically sound & sourced       |

The result is a chatbot that **plans in concepts**, **verifies facts on the fly**, and **speaks only when the user is really done**.

---

## System Overview  <a name="system-overview"></a>

```
User (voice or text)
        â”‚
  Whisper / tokenizer â”€â”€â–º Intent â†’  LCM Planner  â”€â”€â–º Concept plan
        â”‚                               â”‚
        â–¼                               â”‚  (confidence, relevance)
Turnâ€‘taking (VAD)                 Factual Verifier  (E5+YAGO)
        â”‚                               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€ â€œOK / Reviseâ€ â—„â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                Surface Realiser (Flanâ€‘T5â€‘LoRA)
                         â”‚
                    PiperÂ TTS  â†’  Speaker
```

### Core Components

| Layer                 | Openâ€‘source model                       | Size          | Runs on         |
| --------------------- | --------------------------------------- | ------------- | --------------- |
| Planner               | **Meta Large Concept Model (base)**     | 550â€¯MB        | CPU or GPU      |
| Embedding + Retrieval | **intfloat/e5â€‘smallâ€‘v2** + Faiss        | 90â€¯MB         | CPU             |
| FactÂ Verifier         | **microsoft/debertaâ€‘v3â€‘baseâ€‘mnliâ€‘lora** | 360â€¯MB        | GPU recommended |
| Realiser              | **Flanâ€‘T5â€‘Smallâ€¯+â€¯LoRA**                | 300â€¯MB        | CPU or GPU      |
| ASR /Â TTS             | **Whisperâ€‘tiny**, **Piper**             | 70â€¯MB / 50â€¯MB | CPU             |

All models are ApacheÂ 2.0 or CC licensed â€“ fully offlineâ€‘friendly.

---

## QuickÂ Start  <a name="quick-start"></a>

### 1Â Prerequisites

* PythonÂ 3.9+
* `pip install torch==2.2.0` (CUDAÂ 11.8) or CPU build
* â‰ˆâ€¯2â€¯GB VRAM (GPU) **or** 8â€¯GB RAM (CPUâ€‘only)

### 2Â Install

```bash
# clone repo
 git clone https://github.com/yourname/blickshift.git
 cd blickshift

# install deps
 pip install -r requirements.txt

# fetch weights (~1â€¯GB total)
 python scripts/download_weights.py  # resumable
```

### 3Â Textâ€‘only demo

```bash
python blickshift_pipeline.py "Explain why solar costs keep falling"
```

### 4Â Voice chat

```bash
python audio_chat.py --mic 1 --voice
```

Hold `Space` to talk.  Release â†’ the bot thinks, verifies, then answers aloud.

---

## DemoÂ Scenarios  <a name="demo-scenarios"></a>

| Scenario               | What to try                         | Metacognitive moment                                                                     |
| ---------------------- | ----------------------------------- | ---------------------------------------------------------------------------------------- |
| **Factual correction** | â€œHow many Grammys has Madonna won?â€ | Planner says **20**, verifier finds source saying **7** â†’ realiser fixes before speaking |
| **Turnâ€‘taking**        | Keep talking without pause          | VAD shows *low turnâ€‘score*, bot waits vs. interrupting                                   |
| **Offâ€‘topic guard**    | Ask physics after a Python question | Relevance score < 0.3 â†’ bot asks for clarification                                       |

---

## Project Layout  <a name="project-layout"></a>

```
â”œâ”€ data/              # sample KG slices & embeddings
â”œâ”€ scripts/
â”‚   â”œâ”€ download_weights.py
â”‚   â””â”€ prepare_embeddings.py
â”œâ”€ blickshift_pipeline.py     # main text demo
â”œâ”€ audio_chat.py              # ASR â†” TTS loop
â”œâ”€ lcm_pipeline.py            # planner only
â”œâ”€ plan_realiser.py           # planâ†’text demo
â””â”€ notebooks/
    â””â”€ Blickshift_Playground.ipynb
```

---

## ExtendingÂ &Â ResearchÂ Hooks  <a name="extending--research-hooks"></a>

* **Swap planner:** Plug a rule engine (`pyDatalog`) into `PlannerBase` interface.
* **Add citation chains:**  Return the verifierâ€™s top evidence snippets next to each sentence.
* **Fineâ€‘tune realiser style:** Train LoRA on company tone or multiple languages.
* **Integrate gesture signals:** Use webcam to boost turnâ€‘taking heuristics.

---

## FAQ  <a name="faq"></a>

**QÂ Is this productionâ€‘ready?**
*A*Â No, itâ€™s a research prototype â€“ good for demos, papers, or as a teaching tool on metacognition & LLM alignment.

**QÂ How fast is it?**
A typical exchange (3 concept plan) takes **<â€¯600â€¯ms** endâ€‘toâ€‘end on RTXâ€‘3060; **\~1.8â€¯s** CPUâ€‘only.

**QÂ Can I use another LLM as realiser?**
Yes â€“ any seqâ€‘2â€‘seq model (e.g. Mistralâ€‘7Bâ€‘Instruct) that you fineâ€‘tune on `plan â†’ text` pairs.

---

## Roadmap  <a name="roadmap"></a>

| Quarter     | Goal                                                          |
| ----------- | ------------------------------------------------------------- |
| Â Q3Â â€‘Â 2025Â  | Release Docker image; live web demo w/ WebRTC audio           |
| Â Q4Â â€‘Â 2025Â  | Multiâ€‘language support (ESÂ +Â DE) via SONAR fineâ€‘tune          |
| Â Q1Â â€‘Â 2026Â  | Open dataset of **1â€¯M** planâ†”text pairs for realiser training |

---

## License  <a name="license"></a>

Code is **ApacheÂ 2.0**.  Model weights keep original licenses (Meta LCM Research, CCâ€‘BYâ€‘SA for ConceptNet, etc.).  Check `LICENSES/` folder for details.

---

> **Blickshiftâ€™s credo:**  *"Reason in silence, verify with evidence, then speak with confidence."*

